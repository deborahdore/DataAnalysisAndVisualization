{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality Reduction Techniques Notebook\n"
      ],
      "metadata": {
        "id": "Sex7DctO5g9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Introduction to Dimensionality Reduction\n",
        "Dimensionality reduction is the process of reducing the number of input variables or features in a dataset while preserving its essential structure and information.\n",
        "\n",
        "# Why is Dimensionality Reduction Important?\n",
        "1. Simplifies data visualization.\n",
        "2. Reduces computational costs.\n",
        "3. Mitigates the curse of dimensionality.\n",
        "4. Improves model performance by removing irrelevant or redundant features.\n",
        "\n",
        "# Key Techniques\n",
        "1. Principal Component Analysis (PCA).\n",
        "2. Multidimensional Scaling (MDS).\n",
        "3. Locally Linear Embedding (LLE).\n",
        "4. Isomap.\n",
        "5. t-SNE.\n"
      ],
      "metadata": {
        "id": "wf71fPWx5n9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Required Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE, MDS, LocallyLinearEmbedding, Isomap\n",
        "from sklearn.datasets import load_iris, fetch_openml, make_swiss_roll"
      ],
      "metadata": {
        "id": "uGyd7Sb25icx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal Component Analysis (PCA)\n",
        "PCA reduces the dimensionality of data by projecting it onto a set of orthogonal components that maximize variance.\n"
      ],
      "metadata": {
        "id": "fEdA0gx-5wpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-by-Step Example with the Iris Dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target"
      ],
      "metadata": {
        "id": "Q5LeeiqG5xZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "6iZqDUkg537A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)"
      ],
      "metadata": {
        "id": "STsbNxxU55T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "for target, color in zip(np.unique(y), ['r', 'g', 'b']):\n",
        "    plt.scatter(X_pca[y == target, 0], X_pca[y == target, 1], label=data.target_names[target], color=color)\n",
        "plt.title(\"PCA on Iris Dataset\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hlVTJ1s356wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: PCA Implementation\n",
        "\"\"\"\n",
        "Task:\n",
        "1. Choose a different dataset (e.g., Wine dataset from sklearn).\n",
        "2. Perform PCA to reduce the dataset to 2 dimensions.\n",
        "3. Visualize the results with a scatter plot.\n",
        "\n",
        "Hint:\n",
        "Use `load_wine()` from sklearn.datasets.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "w7dMPXtG6AUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## t-SNE\n",
        "t-SNE is a non-linear dimensionality reduction method used primarily for visualization of high-dimensional data.\n"
      ],
      "metadata": {
        "id": "3xBPnDwx6DEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with MNIST Dataset\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load a subset of MNIST dataset\n",
        "# Load MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "X = mnist.data\n",
        "y = mnist.target"
      ],
      "metadata": {
        "id": "XvbleEfX6JBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, init='pca', max_iter=250)\n",
        "X_tsne = tsne.fit_transform(X)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1pHtLJmA6KCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "for digit in np.unique(y):\n",
        "    plt.scatter(X_tsne[y == digit, 0], X_tsne[y == digit, 1], label=digit)\n",
        "plt.title(\"t-SNE Visualization of MNIST\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "liyWfLJw6Lvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: t-SNE Visualization\n",
        "\"\"\"\n",
        "Task:\n",
        "1. Use t-SNE on a different dataset (e.g., Iris dataset).\n",
        "2. Visualize the clusters and interpret the results.\n",
        "\n",
        "Hint:\n",
        "Use the Iris dataset and try different values for `perplexity` (e.g., 5, 30, 50).\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WeLnikxP6gBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Isomap\n",
        "Isomap is a non-linear dimensionality reduction technique that uses geodesic distances to preserve the global structure of the data.\n"
      ],
      "metadata": {
        "id": "mD5HUh-H6i_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with Swiss Roll Dataset\n",
        "X_swiss, _ = make_swiss_roll(n_samples=1000, noise=0.05)"
      ],
      "metadata": {
        "id": "ioY4Nblj6h0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Isomap\n",
        "isomap = Isomap(n_components=2, n_neighbors=10)\n",
        "X_isomap = isomap.fit_transform(X_swiss)"
      ],
      "metadata": {
        "id": "6uWkm3P06nyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the results\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "ax[0].scatter(X_swiss[:, 0], X_swiss[:, 2], c=X_swiss[:, 1], cmap='Spectral')\n",
        "ax[0].set_title(\"Original Swiss Roll\")\n",
        "ax[1].scatter(X_isomap[:, 0], X_isomap[:, 1], c=X_swiss[:, 1], cmap='Spectral')\n",
        "ax[1].set_title(\"Isomap Flattened Representation\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A1Xx-H666pPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Locally Linear Embedding (LLE)\n",
        "LLE is a non-linear dimensionality reduction technique that preserves local relationships among data points."
      ],
      "metadata": {
        "id": "BeQqSkeq6s0M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FLOfF2H5fPa"
      },
      "outputs": [],
      "source": [
        "# Example with Swiss Roll Dataset\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
        "X_lle = lle.fit_transform(X_swiss)\n",
        "\n",
        "# Visualize the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_lle[:, 0], X_lle[:, 1], c=X_swiss[:, 1], cmap='Spectral')\n",
        "plt.title(\"LLE Flattened Representation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4: LLE on Custom Data\n",
        "\"\"\"\n",
        "Task:\n",
        "1. Use LLE on a different dataset (e.g., MNIST or Iris).\n",
        "2. Compare the results with other techniques (e.g., Isomap).\n",
        "\n",
        "Hint:\n",
        "Adjust `n_neighbors` to observe its effect on the output.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "0PtZEiOa602q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}