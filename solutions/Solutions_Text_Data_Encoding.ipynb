{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Data Encoding"
      ],
      "metadata": {
        "id": "VNzZmunVOBOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Required Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.feature_extraction.text as sk_text\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "oIIpsv3aOD8Z"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Corpus\n",
        "corpus = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The dog played in the park.\",\n",
        "    \"Cats and dogs are great pets.\"\n",
        "]"
      ],
      "metadata": {
        "id": "yecZFpc5OEer"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. One-Hot Encoding using Sklearn\n"
      ],
      "metadata": {
        "id": "DACrtpRiOIVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(binary=True)\n",
        "one_hot_encoded = vectorizer.fit_transform(corpus).toarray()\n",
        "print(\"One-Hot Encoded Matrix:\")\n",
        "print(one_hot_encoded)"
      ],
      "metadata": {
        "id": "qk3vjuKyOFcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc8a931-adec-489f-c907-b1cd4f54c053"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-Hot Encoded Matrix:\n",
            "[[0 0 1 0 0 0 0 0 1 1 0 0 0 1 1]\n",
            " [0 0 0 0 1 0 0 1 0 0 1 0 1 0 1]\n",
            " [1 1 0 1 0 1 1 0 0 0 0 1 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise: Make it yourself without using libraries\n",
        "def tokenize(text):\n",
        "    return text.lower().replace(\".\", \"\").replace(\",\", \"\").split()\n",
        "\n",
        "def build_vocab(corpus):\n",
        "    vocab = set()\n",
        "    for sentence in corpus:\n",
        "        vocab.update(tokenize(sentence))\n",
        "    return sorted(vocab)  # Sorting to ensure consistent ordering\n",
        "\n",
        "def one_hot_encoding(corpus):\n",
        "    vocab = build_vocab(corpus)\n",
        "    word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "    one_hot_vectors = []\n",
        "    for sentence in corpus:\n",
        "        tokenized_sentence = tokenize(sentence)\n",
        "        vector = [1 if word in tokenized_sentence else 0 for word in vocab]\n",
        "        one_hot_vectors.append(vector)\n",
        "\n",
        "    return vocab, one_hot_vectors\n",
        "\n",
        "vocab, one_hot_vectors = one_hot_encoding(corpus)\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"One-hot encoded vectors:\")\n",
        "for vector in one_hot_vectors:\n",
        "    print(vector)\n"
      ],
      "metadata": {
        "id": "s7sfRowMOqKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b714472a-5ac7-4560-fbed-19c6dad41e38"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['and', 'are', 'cat', 'cats', 'dog', 'dogs', 'great', 'in', 'mat', 'on', 'park', 'pets', 'played', 'sat', 'the']\n",
            "One-hot encoded vectors:\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1]\n",
            "[0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1]\n",
            "[1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Index-Based Encoding"
      ],
      "metadata": {
        "id": "v0wOF82ZOL03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = {word: idx for idx, word in enumerate(set(\" \".join(corpus).split()))}\n",
        "index_encoded = [[word_to_index[word] for word in sentence.split()] for sentence in corpus]\n",
        "print(\"Index-Based Encoding:\")\n",
        "print(index_encoded)"
      ],
      "metadata": {
        "id": "CZCY8GBgOJ8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c907364a-f23f-40d1-de29-79272d456d8e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index-Based Encoding:\n",
            "[[7, 9, 14, 1, 2, 6], [7, 8, 15, 13, 2, 12], [4, 0, 5, 10, 11, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Bag of Words (using One Hot Encoding)"
      ],
      "metadata": {
        "id": "J0s7CdgZOSeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_vectorizer = CountVectorizer()\n",
        "bow_matrix = bow_vectorizer.fit_transform(corpus).toarray()\n",
        "print(\"\\nBag of Words Matrix:\")\n",
        "print(bow_matrix)"
      ],
      "metadata": {
        "id": "apy0d68VOTUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd8adb5-7a49-4b1b-a163-95dcd5d18e4a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bag of Words Matrix:\n",
            "[[0 0 1 0 0 0 0 0 1 1 0 0 0 1 2]\n",
            " [0 0 0 0 1 0 0 1 0 0 1 0 1 0 2]\n",
            " [1 1 0 1 0 1 1 0 0 0 0 1 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. TF-IDF"
      ],
      "metadata": {
        "id": "iBAmZAbWOdCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_matrix)"
      ],
      "metadata": {
        "id": "kcbRdi_BOOXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bb12366-3152-46be-94ae-a9f4f980a86f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.39798027 0.         0.         0.\n",
            "  0.         0.         0.39798027 0.39798027 0.         0.\n",
            "  0.         0.39798027 0.60534851]\n",
            " [0.         0.         0.         0.         0.39798027 0.\n",
            "  0.         0.39798027 0.         0.         0.39798027 0.\n",
            "  0.39798027 0.         0.60534851]\n",
            " [0.40824829 0.40824829 0.         0.40824829 0.         0.40824829\n",
            "  0.40824829 0.         0.         0.         0.         0.40824829\n",
            "  0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise: Make it yourself without using libraries\n",
        "import math\n",
        "\n",
        "def compute_tf(corpus, vocab):\n",
        "    \"\"\"Computes term frequency (TF) for each document.\"\"\"\n",
        "    tf_list = []\n",
        "    for sentence in corpus:\n",
        "        tokenized_sentence = tokenize(sentence)\n",
        "        tf_dict = {word: tokenized_sentence.count(word) / len(tokenized_sentence) for word in vocab}\n",
        "        tf_list.append(tf_dict)\n",
        "    return tf_list\n",
        "\n",
        "def compute_idf(corpus, vocab):\n",
        "    \"\"\"Computes inverse document frequency (IDF).\"\"\"\n",
        "    num_docs = len(corpus)\n",
        "    idf_dict = {}\n",
        "    for word in vocab:\n",
        "        doc_count = sum(1 for sentence in corpus if word in tokenize(sentence))\n",
        "        idf_dict[word] = math.log((num_docs + 1) / (doc_count + 1)) + 1  # Smoothing\n",
        "    return idf_dict\n",
        "\n",
        "def compute_tfidf(corpus):\n",
        "    \"\"\"Computes TF-IDF vectors for the corpus.\"\"\"\n",
        "    vocab = build_vocab(corpus)\n",
        "    tf_list = compute_tf(corpus, vocab)\n",
        "    idf_dict = compute_idf(corpus, vocab)\n",
        "\n",
        "    tfidf_vectors = []\n",
        "    for tf_dict in tf_list:\n",
        "        tfidf_vectors.append([tf_dict[word] * idf_dict[word] for word in vocab])\n",
        "\n",
        "    return vocab, tfidf_vectors\n",
        "\n",
        "vocab, tfidf_vectors = compute_tfidf(corpus)\n",
        "print(\"Vocabulary:\", vocab, \"\\n\")\n",
        "print(\"TF-IDF encoded vectors:\\n\")\n",
        "for vector in tfidf_vectors:\n",
        "    print(vector, \"\\n\")"
      ],
      "metadata": {
        "id": "XO1jlPb1Oyjc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5638615f-b563-4936-afc7-a4e47f7d46ff"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['and', 'are', 'cat', 'cats', 'dog', 'dogs', 'great', 'in', 'mat', 'on', 'park', 'pets', 'played', 'sat', 'the'] \n",
            "\n",
            "TF-IDF encoded vectors:\n",
            "\n",
            "[0.0, 0.0, 0.2821911967599909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2821911967599909, 0.2821911967599909, 0.0, 0.0, 0.0, 0.2821911967599909, 0.42922735748392693] \n",
            "\n",
            "[0.0, 0.0, 0.0, 0.0, 0.2821911967599909, 0.0, 0.0, 0.2821911967599909, 0.0, 0.0, 0.2821911967599909, 0.0, 0.2821911967599909, 0.0, 0.42922735748392693] \n",
            "\n",
            "[0.2821911967599909, 0.2821911967599909, 0.0, 0.2821911967599909, 0.0, 0.2821911967599909, 0.2821911967599909, 0.0, 0.0, 0.0, 0.0, 0.2821911967599909, 0.0, 0.0, 0.0] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Word2Vec Implementation"
      ],
      "metadata": {
        "id": "abzWf1gQOg1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing sentences\n",
        "sentences = [sentence.lower().split() for sentence in corpus]"
      ],
      "metadata": {
        "id": "imatUGNMOiTP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "a4iXbmvCOjWh"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Get vector for 'cat'\n",
        "print(\"\\nWord2Vec Embedding for 'cat':\")\n",
        "print(word2vec_model.wv['cat'])\n",
        "\n",
        "# Example: Similar words to 'dog'\n",
        "print(\"\\nWords similar to 'dog':\")\n",
        "print(word2vec_model.wv.most_similar('dog'))"
      ],
      "metadata": {
        "id": "a5JXMmyhOkas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58174250-005a-4696-880e-b4b6c8c4459a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word2Vec Embedding for 'cat':\n",
            "[ 0.07898068 -0.06989504 -0.09155865 -0.00355753 -0.03099841  0.07894317\n",
            "  0.05938574 -0.01545663  0.01510963  0.01790041]\n",
            "\n",
            "Words similar to 'dog':\n",
            "[('great', 0.4705304801464081), ('the', 0.4318247437477112), ('and', 0.22384949028491974), ('are', 0.19903580844402313), ('mat.', 0.09823879599571228), ('park.', 0.05098552256822586), ('pets.', 0.03376540169119835), ('dogs', 0.026828058063983917), ('in', -0.08308950811624527), ('on', -0.09835172444581985)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises"
      ],
      "metadata": {
        "id": "N8iOt3dpPvD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise (One-Hot Encoding - Sklearn)\n",
        "\"\"\"\n",
        "Task:\n",
        "Use CountVectorizer with binary=True to apply one-hot encoding on the following sentences:\n",
        "\n",
        "corpus = [\n",
        "    \"I love programming.\",\n",
        "    \"Programming is fun.\",\n",
        "    \"I love fun activities.\"\n",
        "]\n",
        "Print the resulting one-hot encoded matrix.\n",
        "Print the vocabulary mapping.\n",
        "\"\"\"\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Corpus\n",
        "corpus = [\n",
        "    \"I love programming.\",\n",
        "    \"Programming is fun.\",\n",
        "    \"I love fun activities.\"\n",
        "]\n",
        "\n",
        "# Initialize CountVectorizer with binary=True\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "# Fit and transform the corpus\n",
        "one_hot_matrix = vectorizer.fit_transform(corpus).toarray()\n",
        "\n",
        "# Get the vocabulary mapping\n",
        "vocab = vectorizer.vocabulary_\n",
        "\n",
        "# Print results\n",
        "print(\"One-hot encoded matrix:\")\n",
        "print(one_hot_matrix)\n",
        "print(\"\\nVocabulary mapping:\\n\")\n",
        "print(vocab)"
      ],
      "metadata": {
        "id": "9IyBYZobRJx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c9cb6b8-41a5-4fa8-9a81-10cd052dd78e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-hot encoded matrix:\n",
            "[[0 0 0 1 1]\n",
            " [0 1 1 0 1]\n",
            " [1 1 0 1 0]]\n",
            "\n",
            "Vocabulary mapping:\n",
            "\n",
            "{'love': 3, 'programming': 4, 'is': 2, 'fun': 1, 'activities': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise (TF-IDF - Sklearn + Manual Calculation)\n",
        "\"\"\"\n",
        "Task:\n",
        "\n",
        "Compute the TF-IDF matrix using TfidfVectorizer for the following sentences:\n",
        "corpus = [\n",
        "    \"Machine learning is fascinating.\",\n",
        "    \"Deep learning is a subset of machine learning.\",\n",
        "    \"Neural networks are used in deep learning.\"\n",
        "]\n",
        "Manually compute Term Frequency (TF) for the word \"learning\" in each document.\n",
        "Manually compute Inverse Document Frequency (IDF) for \"learning\".\n",
        "Compare the manually computed TF-IDF for \"learning\" with the value from Sklearn.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import math\n",
        "\n",
        "# Define the corpus\n",
        "corpus = [\n",
        "    \"Machine learning is fascinating.\",\n",
        "    \"Deep learning is a subset of machine learning.\",\n",
        "    \"Neural networks are used in deep learning.\"\n",
        "]\n",
        "\n",
        "print(\"## Step 1: Compute TF-IDF matrix using TfidfVectorizer\")\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n",
        "print(\"Feature names:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\n## Step 2: Manually compute Term Frequency (TF) for 'learning'\")\n",
        "\n",
        "def compute_tf(word, doc):\n",
        "    words = doc.lower().split()\n",
        "    return words.count(word.lower()) / len(words)\n",
        "\n",
        "tf_learning = [compute_tf(\"learning\", doc) for doc in corpus]\n",
        "\n",
        "for i, tf in enumerate(tf_learning):\n",
        "    print(f\"TF('learning') in document {i+1}: {tf:.4f}\")\n",
        "\n",
        "print(\"\\n## Step 3: Manually compute Inverse Document Frequency (IDF) for 'learning'\")\n",
        "\n",
        "def compute_idf(word, corpus):\n",
        "    N = len(corpus)\n",
        "    n = sum(1 for doc in corpus if word.lower() in doc.lower())\n",
        "    return (math.log(N + 1 / n + 1)) + 1\n",
        "\n",
        "idf_learning = compute_idf(\"learning\", corpus)\n",
        "print(f\"IDF('learning'): {idf_learning:.4f}\")\n",
        "\n",
        "print(\"\\n## Step 4: Manually compute TF-IDF for 'learning'\")\n",
        "\n",
        "tfidf_learning_manual = [tf * idf_learning for tf in tf_learning]\n",
        "\n",
        "for i, tfidf in enumerate(tfidf_learning_manual):\n",
        "    print(f\"TF-IDF('learning') in document {i+1}: {tfidf:.4f}\")\n",
        "\n",
        "print(\"\\n## Step 5: Compare with sklearn's output\")\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "learning_index = list(feature_names).index('learning')\n",
        "sklearn_tfidf_values = tfidf_matrix.toarray()[:, learning_index]\n",
        "\n",
        "print(\"Sklearn TF-IDF values for 'learning':\")\n",
        "for i, value in enumerate(sklearn_tfidf_values):\n",
        "    print(f\"Document {i+1}: {value:.4f}\")\n",
        "\n",
        "print(\"Manual TF-IDF values:\")\n",
        "print(tfidf_learning_manual)\n",
        "print(\"\\nSklearn TF-IDF values:\")\n",
        "print(sklearn_tfidf_values)\n",
        "\n",
        "print(\"\\nDifferences:\")\n",
        "for i, (manual, sklearn) in enumerate(zip(tfidf_learning_manual, sklearn_tfidf_values)):\n",
        "    print(f\"Document {i+1}: {sklearn - manual:.4f}\")"
      ],
      "metadata": {
        "id": "ra5G5d38Qsso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa3db0de-bc48-48e7-acab-fc71e11b2508"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Step 1: Compute TF-IDF matrix using TfidfVectorizer\n",
            "TF-IDF matrix shape: (3, 12)\n",
            "Feature names: ['are' 'deep' 'fascinating' 'in' 'is' 'learning' 'machine' 'networks'\n",
            " 'neural' 'of' 'subset' 'used']\n",
            "\n",
            "## Step 2: Manually compute Term Frequency (TF) for 'learning'\n",
            "TF('learning') in document 1: 0.2500\n",
            "TF('learning') in document 2: 0.1250\n",
            "TF('learning') in document 3: 0.0000\n",
            "\n",
            "## Step 3: Manually compute Inverse Document Frequency (IDF) for 'learning'\n",
            "IDF('learning'): 2.4663\n",
            "\n",
            "## Step 4: Manually compute TF-IDF for 'learning'\n",
            "TF-IDF('learning') in document 1: 0.6166\n",
            "TF-IDF('learning') in document 2: 0.3083\n",
            "TF-IDF('learning') in document 3: 0.0000\n",
            "\n",
            "## Step 5: Compare with sklearn's output\n",
            "Sklearn TF-IDF values for 'learning':\n",
            "Document 1: 0.3731\n",
            "Document 2: 0.5215\n",
            "Document 3: 0.2426\n",
            "Manual TF-IDF values:\n",
            "[0.6165842671983568, 0.3082921335991784, 0.0]\n",
            "\n",
            "Sklearn TF-IDF values:\n",
            "[0.37311881 0.52150095 0.2425937 ]\n",
            "\n",
            "Differences:\n",
            "Document 1: -0.2435\n",
            "Document 2: 0.2132\n",
            "Document 3: 0.2426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise (Word2Vec - Custom Training & Exploration)\n",
        "\"\"\"\n",
        "Task:\n",
        "\n",
        "Train a Word2Vec model using the following corpus:\n",
        "corpus = [\n",
        "    \"Artificial intelligence is transforming the world.\",\n",
        "    \"Machine learning and deep learning are part of AI.\",\n",
        "    \"Neural networks power many AI applications.\"\n",
        "]\n",
        "Find and print the vector representation of the word \"AI\".\n",
        "Find and print the most similar words to \"learning\".\n",
        "Generate a new sentence and infer its most relevant words based on the trained model.\n",
        "\"\"\"\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "import numpy as np\n",
        "\n",
        "print(\"## Step 1: Prepare the corpus and train the Word2Vec model\")\n",
        "\n",
        "corpus = [\n",
        "    \"Artificial intelligence is transforming the world.\",\n",
        "    \"Machine learning and deep learning are part of AI.\",\n",
        "    \"Neural networks power many AI applications.\"\n",
        "]\n",
        "\n",
        "# Preprocess the corpus\n",
        "processed_corpus = [simple_preprocess(sentence) for sentence in corpus]\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = Word2Vec(sentences=processed_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "\n",
        "print(\"\\n## Step 2: Find and print the vector representation of the word 'AI'\")\n",
        "\n",
        "try:\n",
        "    ai_vector = model.wv['ai']\n",
        "    print(\"Vector representation of 'AI':\")\n",
        "    print(ai_vector)\n",
        "except KeyError:\n",
        "    print(\"The word 'AI' is not in the vocabulary. Try 'artificial' or 'intelligence' instead.\")\n",
        "    print(\"Vector representation of 'artificial':\", model.wv['artificial'])\n",
        "\n",
        "print(\"\\n## Step 3: Find and print the most similar words to 'learning'\")\n",
        "\n",
        "try:\n",
        "    similar_words = model.wv.most_similar('learning', topn=3)\n",
        "    print(\"Most similar words to 'learning':\")\n",
        "    for word, score in similar_words:\n",
        "        print(f\"{word}: {score:.4f}\")\n",
        "except KeyError:\n",
        "    print(\"The word 'learning' is not in the vocabulary or doesn't have enough context.\")\n",
        "\n",
        "print(\"\\n## Step 4: Generate a new sentence and infer its most relevant words\")\n",
        "\n",
        "new_sentence = \"AI systems can process and analyze data quickly.\"\n",
        "processed_sentence = simple_preprocess(new_sentence)\n",
        "\n",
        "# Calculate the average vector for the sentence\n",
        "sentence_vector = np.mean([model.wv[word] for word in processed_sentence if word in model.wv], axis=0)\n",
        "\n",
        "# Find the most similar words to the sentence vector\n",
        "most_relevant_words = model.wv.similar_by_vector(sentence_vector, topn=5)\n",
        "\n",
        "print(\"Generated sentence:\", new_sentence)\n",
        "print(\"Most relevant words based on the trained model:\")\n",
        "for word, score in most_relevant_words:\n",
        "    print(f\"{word}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "iV9PQ8vlQ9AI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5dcd9e4-0e46-4e54-b98c-2dd8ac4f8db9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Step 1: Prepare the corpus and train the Word2Vec model\n",
            "\n",
            "## Step 2: Find and print the vector representation of the word 'AI'\n",
            "Vector representation of 'AI':\n",
            "[-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
            " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
            " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
            " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
            "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
            "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
            "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
            " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
            "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
            "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
            " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
            " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
            "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
            " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
            "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
            " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
            " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
            " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
            " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
            "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
            " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
            " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
            " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
            "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
            " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n",
            "\n",
            "## Step 3: Find and print the most similar words to 'learning'\n",
            "Most similar words to 'learning':\n",
            "of: 0.1607\n",
            "artificial: 0.1592\n",
            "part: 0.1373\n",
            "\n",
            "## Step 4: Generate a new sentence and infer its most relevant words\n",
            "Generated sentence: AI systems can process and analyze data quickly.\n",
            "Most relevant words based on the trained model:\n",
            "and: 0.6771\n",
            "ai: 0.6557\n",
            "of: 0.1076\n",
            "many: 0.0904\n",
            "world: 0.0771\n"
          ]
        }
      ]
    }
  ]
}